---
title: "Melbourne Datathon 2018"
authors: "Charles Galea (s3688570), Sean Smyth (s    ), Nicholas Davis (s    ) and  Luke Daws (s      )"
date: "Sept 2018"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: no
    toc_depth: 3
linkcolor: blue
subtitle: COSC 2669 Legal and Ethical Issues in Data Science
documentclass: article
---


\newpage

\tableofcontents


<!-- -->


\newpage

# Introduction \label{sec1}

The objective of this project was to build classifiers to predict whether a tram/bus or train stop will be busy at a particular time of day. The data were obtained from the Public Transport Victoria and was comprised of MYKI touch on touch off transactions for the previous 3 years. The target feature was based on the number of transactions at given times of the day and were categorized as busy, slightly busy or not busy.

## Descriptive Features

* Calendar Year
* Financial Year
* Financial Month
* Calendar Month
* CalendarMonthSeq
* CalendarQuarter
* FinancialQuarter
* CalendarWeek
* DayType
* DayTypeCategory
* WeekdaySeq
* WeekDay
* FinancialMonthSeq
* FinancialMonthName
* MonthNumber 
* ABSWeek
* QuarterName
* Card_SubType_ID
* Card_SubType_Desc
* Payment_Type
* Fare_Type
* Concession_Type
* MI_Card_Group
* StopLocationID
* StopNameShort
* StopNameLong
* StopType
* SuburbName
* PostCode
* RegionName
* LocalGovernmentArea
* StatDivision
* GPSLat
* GPSLong
* Mode
* BusinessDate    
* DateTime    
* CardID    
* CardType    
* VehicleID    
* ParentRoute    
* RouteID    
* StopID    

The target feature has three classes busy, slightly busy or not busy. To reiterate, the goal is to predict **whether a stop is busy at a particular time of the day**.


# Methodology

## Data pre-processing

Due to size of the dataset we initially sampled the data in order to obain a smaller, more managable, dataset. Datasets containing MYKI transactions, date/time and stop location information were combined and irrelevant features were removed. MYKI transactions at each stop per hour were determined and the final dataset was used for training several machine learning algorithms.

# Install and load required R libraries
```{r, message = FALSE, warning=FALSE}
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre-9.0.4')
library(dplyr)
library(readr)
library(tidyverse)
library(lubridate)
library(mlr)
library(tidyverse) # for ggplot and data wrangly
library(rJava)
library(FSelector) 
library(taRifx)
library(ggplot2)
library(gridExtra)
library(gmodels)
library(GGally)
library(cowplot)
library(tidyr)
library(magrittr)
library(moments)
library(purrr)
library(data.table)
library(latex2exp)
library(caret)
library(robustHD)
library(spFSR)
library(rjson)
library(party)
library(knitr)
library(kableExtra)
library(stringr)
library(mlbench)
library(e1071)
library(MASS)
library(ggmap)
set.seed(999) 
```

# Load Public Holidays Dataset
```{r}
pubHol <- read.csv("publicHol.csv")
```

```{r}
pubHol$Date <- ymd(pubHol$Date)
```

# Load Calander Dataset
```{r}
calendar <- fread('D:\\Datathon\\MelbDatathon2018\\calendar.txt')
colnames(calendar) <- c('', 'Date', 'CalendarYear', 'FinancialYear', 'FinancialMonth', 'CalendarMonth', 'CalendarMonthSeq', 'CalendarQuarter', 'FinancialQuarter', 'CalendarWeek', 'FinancialWeek', 'DayType', 'DayTypeCategory', 'WeekdaySeq', 'WeekDay', 'Day', 'FinancialMonthSeq', 'FinancialMonthName', 'MonthNumber', 'ABSWeek', 'WeekEnding', 'QuarterName')
calendar <- calendar[,-1]
```


# Load Card Types Dataset
```{r}
card_types <- fread('D:\\Datathon\\MelbDatathon2018\\card_types.txt')
colnames(card_types) <- c('CardType', 'Card_SubType_Desc', 'Payment_Type', 'Fare_Type', 'Concession_Type', 'MI_Card_Group')
```

# Load Stop Locations dataset
```{r}
stop_loc <- fread('D:\\Datathon\\MelbDatathon2018\\stop_locations.txt')
colnames(stop_loc) <- c('StopID', 'StopNameShort', 'StopNameLong', 'StopType', 'SuburbName', 'PostCode', 'RegionName', 'LocalGovernmentArea', 'StatDivision', 'GPSLat', 'GPSLong')
```

# Read in and combine folder IDs from different directories
```{r}
ScanOnFolderMaster <- 'D:\\Datathon\\MelbDatathon2018\\Samp_X\\ScanOnTransaction'
ScanOffFolderMaster <- 'D:\\Datathon\\MelbDatathon2018\\Samp_X\\ScanOffTransaction'

mySamp <- 0
condDatON <- FALSE
condDatOFF <- FALSE

ScanOnFolder <- sub("X",mySamp,ScanOnFolderMaster)
ScanOffFolder <- sub("X",mySamp,ScanOffFolderMaster)
onFiles <- list.files(ScanOnFolder,recursive = TRUE,full.names = TRUE, pattern = "\\.txt$")
offFiles <- list.files(ScanOffFolder,recursive = TRUE,full.names = TRUE, pattern = "\\.txt$")
```

#Determine the total number of files 
```{r}
allFiles <- union(onFiles,offFiles)
cat("\nthere are", length(allFiles),'files')
```

# Take a sample of the files in the datathon dataset and then take a sample of rows from each file
```{r}
#myFiles <- onFiles[1:5]

# Take random sample of files and save in file named myFiles
myFiles <- sample(onFiles, 20, replace = FALSE)

#Extract information for trains, trams or buses from files 
first <- TRUE
count <- 0

for (myOn in myFiles){
  #cmd <- paste0("gzip -dc ", myOn)

  dt <- fread(myOn, na.strings="")
  
  #Choose files(train, tram or bus) to extract
  dt_train <- subset(dt, V1 == 2)
  #dt_tram <- subset(dt, V1 == 3)
  #dt_bus <- subset(dt, V1 == 1)
  #dt_train <- subset(dt_train, V7 != "Headless Mode")
 
  #stack the records together
  if (first == TRUE){
    #Randomly sample a given number of rows of data
    allON <- dt_train[sample(nrow(dt_train), 3000), ]
    first <- FALSE
  } else {
    l = list(dt_train,allON)
    allON <- rbindlist(l)
  }
  count <- count + 1
  cat('\n',count,' of ',length(myFiles))
}

cat('\n there are ', format(nrow(allON),big.mark = ","),'rows')

#Add column names
colnames(allON) <- c('Mode','Date','DateTime','CardID','CardType','VehicleID','ParentRoute','RouteID','StopID')
```

# Summary of allON table
```{r}
summary(allON)
```

# Remove Mode, ParentRoute and VehicleID features
```{r}
allON <- allON[,-c(6,7)]
allON <- allON[,-1]
```


# One field is clearly a date time
```{r}
allON[,DateTime := as.POSIXct(DateTime,tz='Australia/Sydney')]
allON[,unixTime := as.numeric(DateTime)]
```

# Combine allON and Card Type datasets
```{r}
allON_card <- left_join(allON, card_types, by = "CardType")
```

# Combine with Stop Locations dataset (Greater Metro district only)
```{r}
allON_card_Loc <- left_join(allON_card, stop_loc, by = "StopID")
allON_card_Loc <- subset(allON_card_Loc, allON_card_Loc$StatDivision == "Greater Metro")
```

# Combine with Calendar dataset
```{r}
allON_card_Loc_Cal <- left_join(allON_card_Loc, calendar, by = "Date")
```

# Dimensions of entire dataset
```{r}
dim(allON_card_Loc_Cal)
```

# Determine the number of tap on transactions at each Stop along each Route per Hour.
```{r}
meta <- allON_card_Loc_Cal %>% group_by(DateTime, FinancialMonth, FinancialWeek, WeekDay, Hour = cut(allON_card_Loc_Cal$DateTime, breaks= "1 hour", labels=FALSE, ordered_result = TRUE), RouteID, StopID) %>% summarize(Number = n())
```

# Add location data and stop type to table
```{r}
loc <- data.frame(StopID = stop_loc$StopID, StopType = stop_loc$StopType, SuburbName = stop_loc$SuburbName, PostCode = stop_loc$PostCode, LocalGovernmentArea = stop_loc$LocalGovernmentArea)
```

# Combine transaction (meta) table with vehicleID (vech) and stop (loc) tables
```{r}
meta_loc <- left_join(meta, loc, by = "StopID")
meta_loc2 <- meta_loc[1:5000,]
```

# Determine the number of MYKI transactions at each stop per hour
```{r}
x <- TRUE
while (length(meta_loc2$StopID) >= 1) {
  num <- meta_loc2[1, 'StopID']
  num <- as.numeric(num)
  stop_trains <- meta_loc2[meta_loc2$StopID == num, ]
  sort(stop_trains$DateTime)
  allONBreaks <- data.frame(stop_trains, cuts = cut(stop_trains$DateTime, breaks= "1 hour", labels=FALSE, ordered_result = TRUE))
  myDates <- table(cut(allONBreaks$DateTime, breaks = "hour"))
  myDatesN <- as.data.frame(myDates)
  myDatesN <- merge(myDatesN, stop_trains)
  #vec <- rep(myDatesN$Freq)
  #vec <-  as.numeric(vec)
  #MydatesNew <- cbind(myDatesN,vec)
  #myDatesN <- as.data.frame(myDatesN)
  #allONBreaks <- as.data.frame(allONBreaks)
  #newDat <- merge(myDatesN, allONBreaks)

  if (x == TRUE) {
    all_comb <- myDatesN
    x <- FALSE
  } else {
    all_comb <- rbind(all_comb, myDatesN)
  }
  meta_loc2 <- meta_loc2[meta_loc2$StopID != num, ]
  }
  
```

# Plot the distribution of MYKI transactions at each stop per hour in the sampled dataset
```{r}
ggplot(all_comb, aes(x = all_comb$Freq)) + geom_histogram(fill = "cyan", colour = "black")
```

# Set a threshold of MYKI transaction per hour to define 'Busy', 'Slightly Busy' and 'Not Busy' stops
```{r}
counts <- 0
for(status in all_comb){
status <- ifelse(all_comb$Freq >= 15, "Busy", ifelse(all_comb$Freq < 15 & all_comb$Freq > 5 , "Slightly Busy", "Not Busy"))
}

mynewDates <- cbind(all_comb, status) 
locData <- data.frame("StopID" = stop_loc$StopID, "Lat" = stop_loc$GPSLat, "Long" = stop_loc$GPSLong)
melbDatathonData <- left_join(mynewDates, locData, by = "StopID")
```

# Write output to csv file. This file will be used for training the machine learning algorithm.
```{r}
write.csv(melbDatathonData, file = "melbDatathonSampleData.csv")
```

# Plot distribution of MYKI transaction per hour highlighting the tresholds 'Busy', 'Slightly Busy' and 'Not Busy' in different colours
```{r}
ggplot(melbDatathonData, aes(x = melbDatathonData$Freq, fill = melbDatathonData$status)) + geom_histogram(colour = "black") + labs(x = "Number of MYKI Transactions", y = "Frequency", label = "Threshold")
```




