---
title: 'Prediction of MYKI Transactions Usage across Melbourne'
subtitle: 'COSC2669 Legal and Ethical Issue in Data Science'
authors: "Charles Galea (s3688570),   "
date: September 2018
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: no
    toc_depth: 3
linkcolor: blue
documentclass: article
references:
- id: knitr
  author:
  - given: YiHui
    family: Xie
    title: Dynamic Documents with R and knitr
    publisher: Chapman and Hall/CRC
    issued:
    - year: 2015
- id: Breiman 
  title: Random Forests
  author:
  - family: Breiman
    given: L.
  issued:
  - year: 2001
  volume: 45(1)
  pages: 5-32
  publisher: Machine Learning 
- id: mlr
  title: "`mlr`: Machine Learning in R"
  author:
  - family: Bischl
    given: Bernd
  - family: Lang
    given: Michel
  - family: Kotthoff
    given: Lars
  - family: Schiffner
    given: Julia
  - family: Richter
    given: Jakob
  - family: Studerus
    given: Erich
  - family: Casalicchio
    given: Giuseppe 
  - family: Jones
    given: Zachary M.
  url: http://jmlr.org/papers/v17/15-066.html
  issued:
  - year: 2016
  volume: 17
  pages: 1-5
  publisher: Journal of Machine Learning Research
---

\newpage

\tableofcontents

\newpage

# Introduction \label{sec1}

The objective of this project was to build classifiers to predict whether an frequency of MYKI transactions for various stops within the Melbourne metropolitian area based on MYKI data collected from 2015-2018. Section 1 describes an overview of our methodology. Section 2 discusses the classifiersâ€™ fine-tuning process and detailed performance analysis of each classifier. Section 3 compares the performance of the classifiers using the same resampling method. Section 4 critiques our methodology. The last section concludes with a summary.

# Methodology

We considered three classifiers - Naive Bayes (NB), Random Forest (RF), and $K$-Nearest Neighbour (KNN). The NB was the baseline classifier. Each classifier was trained to make probability predictions so that we were able to adjust the prediction threshold to refine the performance. We split the full data set into 70 % training set and 30 % test set. Each set resembled the full data by having the same proportion of target classes i.e. approximately 15 % as Busy (Stops with 15 or more MYKI transactions per hour) and 85 % as Not Busy (less than 15 MYKI transactions per hour). For the fine-tuning process, we ran a five-folded cross-validation stratified sampling on each classifier. Stratified sampling was used to cater for the class imbalance of the target feature.

Next, for each classifer, we determined the optimal probability threshold. Using the tuned hyperparameters and the optimal thresholds, we made predictions on the test data. During model training (hyperparameter tuning and threshold adjustment), we relied on mean misclassification error rate (mmce). In addition to mmce, we also used the confusion matrix on the test data to evaluate classifiers' performance. The modelling was implemented in `R` with the `mlr` package [@mlr].

# Hyperparameter Fine-Tuning

## Naive Bayes

Since the training set might have unwittingly excluded rare instances, the NB classifier may produce some fitted zero probabilities as predictions. To mitigate this, we ran a grid search to determine the optimal value of the Laplacian smoothing parameter. Using the stratified sampling discussed in the previous section, we experimented using values ranging from 0 to 30.


The optimal Laplacian parameter was 3.33 with a mean test error of 0.167.

## Random Forest

We tune-fined the number of variables randomly sampled as candidates at each split (i.e. `mtry`). For a classification problem, @Breiman suggests `mtry` = $\sqrt{p}$ where $p$ is the number of descriptive features. In our case, $\sqrt{p} = \sqrt{11}=3.31$. Therefore, we experimented with `mtry` = 2, 3, and 4. We left other hyperparameters, such as the number of trees to grow at the default value. The result was 3 with a mean test error of 0.139.

## $K$-Nearest Neighbour

By using the optimal kernel, we ran a grid search on $k=2,3,...20$. The outcome was 20 with a mean test error of 0.165.

## Feature Selection

Feature selection was used to identify an optimal subset of the available features. Selecting a subset of relevant features can make machine learning algorithm training faster, reduce complexity of the model, improve accuracy and reduce overfitting.There are three broard categoried of feature selection methods: filter methods, wrapper methods and embedded methods.  

### Filter method

The fiter method assigns an importance value to each feature. Based on these values the features are ranked and a feature subset is selected. The learner was fused with the filter method for training of each classification model.

### Wrapper method

The wrapper method used the performance of a learning classifier to access the usefulness of the feature set. In order to select a feature subset the learner was trained repeatedly on different fleature subsets and the subset which lead to the best learner performance was chosen.

# Load R packages
```{r, message=FALSE, warning = FALSE, echo=FALSE, cache = TRUE}
#Load libraries
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre-9.0.4')
library(rJava)
library(magrittr)
library(gridExtra)
library(FSelector) 
library(readr)
library(dplyr)
library(purrr)
library(mlr)
library(data.table)
library(tidyverse)
library(ggplot2)
library(latex2exp)
library(cowplot)
library(caret)
library(robustHD)
library(rjson)
library(party)
library(knitr)
library(kableExtra)
library(stringr)
```

## Load dataset
Loaded dataset and removed redundant index column. 
```{r}
data <- read.csv('D:\\Datathon\\MelbDatathon2018\\MelbDatathon2018_2\\MelbDatathon2018\\melbDatathonSampleData.csv', stringsAsFactors = FALSE)
data <- data[sample(nrow(data), 3000), ]
data <- data[,-1]
#data[, sapply(data, is.character)] <- lapply( data[, sapply(data, is.character )], factor) 
#data$FBS<-as.numeric(data$FBS)
str(data)
```

## Data processing
Printed a summary of the dataset to ensure there were no missing values. 
```{r}
data[data$status == "Slightly Busy",15] <- "Not Busy"
summarizeColumns(data) %>% knitr::kable( caption =  'Feature Summary')
```


# Removed Var1, Freq, DateTime and StopType features.Var1 and DateTime have too many levels. The target features 'status' were derived from 'Freq' there this feature was removed.
```{r}
data <- data[,-c(1:3, 11)]
glimpse(data)
```

#Plotted proportion of 'Busy' and 'Not Busy' target levels for each (i) local government area, (ii) route ID and (iii) hour.
```{r echo = FALSE, fig.width=10, fig.height=10}
p1 <- ggplot(data, aes(x = LocalGovernmentArea, fill = status)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) + xlab("LocalGovernmentArea") + theme(axis.title.y=element_text(size=20,face="bold"))

p2 <- ggplot(data, aes(x = RouteID, fill = status)) + geom_bar() + theme(axis.title.y=element_text(size=20,face="bold")) + scale_x_continuous(limits=c(0, 21), breaks=seq(20))

p3 <- ggplot(data, aes(x = Hour, fill = status)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) + xlab("Hour") + theme(axis.title.y=element_text(size=20,face="bold"))

#p <- plot_grid(p1, p2, p3, p4)
#label <- substitute(paste("Bar Charts for MYKI Transactions"))
#title <- ggdraw() + draw_label(label, fontface='bold', size = 24)
grid.arrange(p1, p2, p3, ncol = 1, top = textGrob("Frequency of MYKI Transactions", gp = gpar(fontface = 3, fontsize = 18), hjust = 0.5, x = 0.5))
```

#Interactive plot of the proportion of 'Busy' and 'Not Busy' target levels for each suburb.
```{r}
library(plotly)
p4 <- ggplot(data, aes(x = SuburbName, fill = status)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) + xlab("Suburb")
gg1 <- ggplotly(p4)
gg1
```

# Plotted the proportion of 'Busy' and 'Not Busy' target levels for the entire dataset.
```{r}
ggplot(data, aes(x = status)) + geom_bar() + labs(x = "Status", y = "Frequency") 
```

# Update status (target) variables
```{r}
data$status[data$status == 'Not Busy'] <- 'Not_Busy'
```

# Dummify categorical variables (categorical variable were re-classified as factors and then converted to numerical variables).
```{r}
data$SuburbName <- data$SuburbName %>%  as.factor()
data$LocalGovernmentArea <- data$LocalGovernmentArea %>% as.factor()
data$StopID <- data$StopID %>% as.factor()
data$PostCode <- data$PostCode %>% as.factor()
data <- createDummyFeatures(data, target = "status", method = '1-of-n')
```

##Shuffle rows prior to splitting dataset
To remove any patterns in the dataset (e.g. due to yearly flucations) To avoid any biases  the rows in the dataset were randomized prior to splitting the data into training and test sets.
```{r}
#Shuffle dataset rows
set.seed(1234)
n <- nrow(data)
shuffled_data <- data[sample(n), ]
```

## Threshold Adjustment
The data was split to obtain the training (70 %) and test (30 %) datasets.
```{r}
# Old school way to spliting the data into 70 % training & 30 % test data
# This is not stratified sampling, which shall be used in model training
# obtain index for training and test indices
training_index <- sample(nrow(shuffled_data)*0.70)
test_index     <- setdiff(seq(1:nrow(shuffled_data)), training_index )

# Get the training data and test data
training_data  <- shuffled_data[training_index, ]
test_data      <- shuffled_data[test_index, ]

```

## Determine the proportion of disease category in the test and training datasets.
```{r echo = FALSE, warning = FALSE}
prop.table(table(training_data$status))
prop.table(table(test_data$status))
```
They were not balanced but were representative of the full dataset. We shall use training data for modeling and test data for model evaluation.

# 2. Modeling ----
# 2.1. Basic configuration ----
```{r}
# Configure classification task
task <- makeClassifTask(data = training_data, target = 'status', id = 'trans_freq', positive = "Busy")

# Configure learners with probability type
learner1 <- makeLearner('classif.naiveBayes', predict.type = 'prob')    # Baseline learner
learner2 <- makeLearner('classif.randomForest', predict.type = 'prob')  # Random Forest learner
learner3 <- makeLearner('classif.kknn', predict.type = 'prob')          # kNN learner
```

# 2.2 Model fine-tuning ----
```{r}
# For naiveBayes, we can fine-tune Laplacian
ps1 <- makeParamSet(makeNumericParam('laplace', lower = 0, upper = 30))

# For randomForest, we can fine-tune mtry i.e mumber of variables randomly 
# sampled as candidates at each split. Following
# Breiman, L. (2001), Random Forests, Machine Learning 45(1), 5-32,
# we can try mtry = 2, 3, 4 as mtry = sqrt(p) where p = 11
ps2 <- makeParamSet(
  makeDiscreteParam('mtry', values = c(2,3,4))
)

# For kknn, we can fine-tune k = 2 to 20 
ps3 <- makeParamSet(
  makeDiscreteParam('k', values = seq(2, 20, by = 1))
)
```

### Configure the tune control search and a 5-CV stratified sampling scheme

```{r echo = FALSE, warning = FALSE}
ctrl <- makeTuneControlGrid()
rdesc <- makeResampleDesc("CV", iters = 5L, stratify = TRUE)
```

### Configure the tune wrapper with tune-tuning settings
```{r echo = FALSE, warning = FALSE}
tunedLearner1 <- makeTuneWrapper(learner1, rdesc, mmce, ps1, ctrl)
tunedLearner2 <- makeTuneWrapper(learner2, rdesc, mmce, ps2, ctrl)
tunedLearner3 <- makeTuneWrapper(learner3, rdesc, mmce, ps3, ctrl)
```

## Model Training

### Train the tune wrappers
```{r echo = FALSE, warning = FALSE}
tunedMod1 <- mlr::train(tunedLearner1, task)
tunedMod2 <- mlr::train(tunedLearner2, task)
tunedMod3 <- mlr::train(tunedLearner3, task)
```

## Model Prediction

### Predict on training data
```{r echo = FALSE, warning = FALSE}
tunedPred1 <- predict(tunedMod1, task)
tunedPred2 <- predict(tunedMod2, task)
tunedPred3 <- predict(tunedMod3, task)
```

## Model Evaluation

### Obtain threshold values for each learner
```{r echo = FALSE, warning = FALSE}
d1 <- generateThreshVsPerfData(tunedPred1, measures = list(mmce))

d2 <- generateThreshVsPerfData(tunedPred2, measures = list(mmce))

d3 <- generateThreshVsPerfData(tunedPred3, measures = list(mmce))

d4 <- generateThreshVsPerfData(list(Naive_Bayes = tunedPred1, Random_Forest = tunedPred2, kNN = tunedPred3), measures = list(mmce))

plotThreshVsPerf(d4) + labs(title = 'Threshold Adjustment', x = 'Threshold')
```

*Figure 6.* Plot for the optimization of the threshold for the kNN, Naive Bayes and Random Forest classifiers trained on all features.

```{r echo = FALSE, warning = FALSE}
# Get threshold for each learner
threshold1 <- d1$data$threshold[which.min(d1$data$mmce)]
threshold2 <- d1$data$threshold[which.min(d2$data$mmce)]
threshold3 <- d1$data$threshold[which.min(d3$data$mmce)]
```


## Evaluation on test data

```{r echo = FALSE, warning = FALSE}
# Use tuned wrapper models and optimal thresholds from previous sections
testPred1 <- predict(tunedMod1, newdata = test_data)
testPred2 <- predict(tunedMod2, newdata = test_data)
testPred3 <- predict(tunedMod3, newdata = test_data)
```

### AUC plots for Naive Bayes, Random Forest and kNN models
Evaluated each algorithm by comparing the plotted AUC curves.
```{r echo = FALSE, warning = FALSE}
dAUC <- generateThreshVsPerfData(list(Naive_Bayes = testPred1, Random_Forest = testPred2, kNN = testPred3), measures = list(fpr, tpr, mmce))

plotROCCurves(dAUC) + labs(title = 'AUC plot', x = 'False Positive Rate', y = 'True Positive Rate')
```

*Figure 7.* AUC plot for the kNN, Naive Bayes and Random Forest classifers trained on all features.

The AUC plots were similar for the kNN and Random Forest classifiers trained on all features (Fig. 7).  

### Performance for Naive Bayes model

#### Misclassification Error and AUC value
```{r echo = FALSE, warning = FALSE}
Naive_Bayes <- performance(testPred1, measures = list(mmce, auc))
Random_Forest <- performance(testPred2, measures = list(mmce, auc))
kNN <- performance(testPred3, measures = list(mmce, auc))

data_p <- data.frame(Naive_Bayes, Random_Forest, kNN)

kable(data_p, caption = "Table 4. Performance for Naive Bayes, Random Forest and kNN classifiers", col.names = c("Naive Bayes", "Random Forest", "kNN")) %>% kable_styling( full_width = F, font_size = 12)
```


The Random Forest classifier performed the best, when the models were trained on all features, with a mean misclassification error of 0.142 and auc value of 0.844 (Table 4). 

#### Confusion Matrix, Precision and Recall for Naive Bayes
```{r echo = FALSE, warning = FALSE}
calculateROCMeasures(testPred1)
```


#### Confusion Matrix, Precision and Recall for Random Forest model
```{r echo = FALSE, warning = FALSE}
calculateROCMeasures(testPred2)
```

#### Confusion Matrix, Precision and Recall for k-Nearest Neighbours model
```{r echo = FALSE, warning = FALSE}
calculateROCMeasures(testPred3)
```

# Feature selection

## Random forest filter method for feature selection

Filter methods assign an importance to each feature. The feature is ranked according to importance value resulting in a feature subset. Create an object named mfv by calling generateFilterValuesData from mlr on classif.task and using the filter method randomForest.importance.
```{r echo = FALSE, warning = FALSE}
# Configure a classification task and specify Disease as the target feature.
shuffled_data2 <- shuffled_data
shuffled_data2$status <- as.numeric(as.factor(shuffled_data2$status))
regressionTask <- makeRegrTask(id = 'datathon', data = shuffled_data2, target = 'status')
regressionTask 
```

### Plot filtered features obtained using information gain and chi squared methods
```{r echo = FALSE, warning = FALSE}
shuffled_data2$status <- as.factor(as.character(shuffled_data2$status))
classif.task <- makeClassifTask(id ='datathon', data = shuffled_data2, target = 'status')
mFV <- generateFilterValuesData(classif.task, method = c('information.gain', 'chi.squared'))
plotFilterValues(mFV) + coord_flip()
```

*Figure 21.* Features selected using a filter selection algorithm in mlr based on (Left panel) information gain and (right panel) chi squared (https://mlr-org.github.io/mlr/articles/tutorial/devel/feature_selection.html).[@mlr]



### Fuse the random forest learner with the information gain filter

We now 'fused' the random forest classification learner with the information.gain filter to train the model.

```{r echo = FALSE, warning = FALSE}
lrn <- makeFilterWrapper(learner = "classif.randomForest", fw.method = "information.gain")
```

### Determine optimal number of features to keep

The optimal percentage of features to keep was determined by 5-fold cross-validation. We use 'information gain' as an importance measure and select the 10 features with highest importance. In each resampling iteration feature selection is carried out on the corresponding training data set before fitting the learner.

```{r echo = FALSE, warning = FALSE}
ps <- makeParamSet(makeDiscreteParam("fw.abs", values = 10))
rdesc <- makeResampleDesc("CV", iters=5)
res <- tuneParams(lrn, task = classif.task, resampling = rdesc, par.set = ps, control = makeTuneControlGrid())
```

### Performance (misclassification error)
The optimal percentage and corresponding misclassification error are:

```{r echo = FALSE, warning = FALSE}
res$x
res$y
```
### Fuse learner with feature selection
We can now fuse it with fw percentage by "wrapper" the random forest learner with the chi-squared method before training the model:

```{r echo = FALSE, warning = FALSE}
fusedLrn <- makeFilterWrapper(learner = "classif.randomForest", fw.method = "chi.squared", fw.abs = res$x$fw.abs)
classif.task <- makeClassifTask(data = training_data, target = "status")
mod <- mlr::train(fusedLrn, classif.task)
```


### View selected features

Now applied getFilteredFeatures on the trained model to view the selected features.

```{r echo = FALSE, warning = FALSE}
Filter <- getFilteredFeatures(mod)
```

## Wrapper Methods

### Select optimal features to use
Used a random search with ten iterations on the random forest classifier and classif.task.

```{r echo = FALSE, warning = FALSE}
randomCtr1_wrap <- makeFeatSelControlRandom(maxit = 10L)
rdesc_wrap <- makeResampleDesc("CV", iters = 3)
sfeats_wrap <- selectFeatures(learner = "classif.randomForest", task = classif.task, resampling = rdesc_wrap, control = randomCtr1_wrap, show.info = FALSE)
```

### Performance (misclassification error)

```{r echo = FALSE, warning = FALSE}
sfeats_wrap$y
```

### View the important features

```{r echo = FALSE, warning = FALSE}
Wrapper <- sfeats_wrap$x
data_Filt_Wrap <- data.frame(Filter, Wrapper[1:10])
colnames(data_Filt_Wrap) <- c('Filter', 'Wrapper')
write.csv(data_Filt_Wrap, "RF_wrapper.csv")

kable(data_Filt_Wrap, caption = "Table 9. Selected Features for Random Forest classifier with mlr Feature Selection (selectFeatures)") %>% kable_styling(full_width = F, font_size = 12)
```


### Wrap feature selection method with learner

By comparing the misclassification error rates, a random search wrapper method out performed the chi squared (filtered) method. We then fused the wrapper method in a learnerusing makeFeatSelWrapper together with makeFeatSelControlRandom and makeResampleDesc objects.

```{r echo = FALSE, warning = FALSE}
lrn_rf_wrapper <- makeLearner(cl = "classif.randomForest", predict.type = 'prob')

lrn_wrapper <- makeFeatSelWrapper(lrn_rf_wrapper, resampling = rdesc_wrap, control = randomCtr1_wrap, show.info = FALSE)

mod_wrapper <- mlr::train(lrn_wrapper, task = classif.task)
```

### Define the test data

```{r echo = FALSE, warning = FALSE}
task_test <- makeClassifTask(data = test_data, target = 'status')
```

## Prediction

```{r echo = FALSE, warning = FALSE}
pred_on_test_wrapper = predict(object = mod_wrapper, task = task_test)
```

## Evaluation
Obtain the confusion matrix by running calculateConfusionMatrix(pred_on_test) and get the ROC.

### Confusion Matrix

```{r echo = FALSE, warning = FALSE}
#Confusion matrix 
calculateConfusionMatrix(pred_on_test_wrapper)
```
### AUC plot

```{r echo = FALSE, warning = FALSE}
d_wrapper = generateThreshVsPerfData(list(RF_wrapper = pred_on_test_wrapper, RF = testPred2), measures = list(fpr, tpr, mmce))

plotROCCurves(d_wrapper)
```

*Figure 22.* AUC plot for the random forest classifer. RF - Random forest classifier trained on all features; RF_spsa_tuned - Random forest classifier with tuned hyperparameters and trained on features selected using the spFRS algorithm; RF_wrapper - Random forest classifier with tned parameters and trained on features selected using the selectFeatures mlr algorithm. 

The AUC plot show that the random forest classifier with optimized hyperparameters and using the wrapper method for feature selection performed similarly. 


### Performance for random forest with mlr feature selection

#### Misclassification Error and AUC value

```{r echo = FALSE, warning = FALSE}
kable(performance(pred_on_test_wrapper, measures = list(mmce, auc)), caption = "Table 10. Classifier Performance") %>% kable_styling(full_width = F)
```
The random forest classifier using the wrapper method for feature selection had a mean misclassification error of 0.216 and AUC value of 0.860 (Table 10).


### Performance for random forest with mlr feature selection

```{r echo = FALSE, warning = FALSE}
calculateROCMeasures(pred_on_test_wrapper)
```

# Discussion



# Conclusion



# References

